{"cells":[{"cell_type":"markdown","metadata":{},"source":["# vLLM Iteration-Level Schedule Optimization\n","## Ari Singer, Jack Holland and Vishwa Ramanakumar\n","\n","This notebook is used to evaluate the performance of our custom vLLM schedule algorithm. It is designed to be run in Google Colab on an A100 instance, as the other instances available do not have enough capacity to fit the LLM used in our experimentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PgpFiQ3TvrXo"},"outputs":[],"source":["# This block mounts your google drive and sets up the ssh key for pulling from git\n","# Will need to set up ssh key yourself\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","! mkdir -p /root/.ssh/\n","%cd /content/drive/MyDrive/eecs598_genai\n","! cp ./.ssh/* /root/.ssh/\n","! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n","! chmod go-rwx /root/.ssh/id_rsa\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713217698104,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"XRrD38SctWXy"},"outputs":[],"source":["# Setup various paths\n","base_drive_path=\"/content/drive/MyDrive/eecs598_genai\"\n","base_github_path=\"/content/github_repo\"\n","python_path=f\"{base_github_path}/python\"\n","vllm_path=f\"{python_path}/vllm_custom\"\n","output_path=f\"{base_drive_path}/output\"\n","data_path=f\"{base_github_path}/data\"\n","requirements_path=f\"{base_drive_path}/colab_requirements/on_disk_requirements.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5198,"status":"ok","timestamp":1713201947279,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"sDxKO1dyts8-","outputId":"d5ded865-ef5a-4cba-e278-8608465bc14e"},"outputs":[],"source":["# Initial clone of github repo\n","%cd /content\n","! git clone --depth 1 -b main git@github.com:ajsinger1/eecs598-genai-project.git github_repo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519093,"status":"ok","timestamp":1713202466369,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"jwLdZxXiQWdi","outputId":"58534898-af02-4c9c-9f81-1b15327cbe33"},"outputs":[],"source":["# Installing dependencies (note that we are using the custom vllm), this should take ~10+ min\n","!pip install --upgrade pip\n","!pip uninstall -y torchaudio torchdata torchtext torchvision\n","!pip install -e {vllm_path} torchaudio torchdata torchtext torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713217698104,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"ucLfMguwV0rZ"},"outputs":[],"source":["# Adding our custom python module directory to the path\n","import sys\n","sys.path.append(f\"{python_path}\")\n","sys.path.append(f\"{vllm_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4211,"status":"ok","timestamp":1713217702312,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"TnLqe_5Hv6Zg","outputId":"392fe58b-20cb-41b9-df1e-e683f2572d7e"},"outputs":[],"source":["# Imports \n","import vllm\n","import pandas as pd\n","from llama_chat_helpers import format_prompt\n","from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n","import gc\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713217702313,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"shX56kiLUBqc","outputId":"03000e4a-3ad2-403c-8314-b65655288c3a"},"outputs":[],"source":["# Loading prompt data as a pandas DataFrame\n","data = pd.read_json(f\"{data_path}/prompts.json\")\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19222,"status":"ok","timestamp":1713215788448,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"361IYWHOUJoA","outputId":"08e2820f-203a-4bcd-cd69-0c656d065b04"},"outputs":[],"source":["# Initializing LLM (Llama 7B Chat)\n","model = \"meta-llama/Llama-2-7b-chat-hf\"\n","llm = vllm.LLM(model=model, preemption_threshold=600, preemption_mode_upper_threshold=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713217702313,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"69s5_K6vekGp","outputId":"7cee97ca-e4a2-48fe-d7fc-fcd4fc62619b"},"outputs":[],"source":["# Setup prompts\n","prompts = data['prompt'].map(format_prompt)\n","print(prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1381703,"status":"ok","timestamp":1713217173388,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"k9rxf12Cen-j","outputId":"658efe76-b771-45a7-f2f7-bf1285305a86"},"outputs":[],"source":["import os\n","\n","model = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","preemption_thresholds = [200, 400, 600, 800, 1000]\n","preemption_upper_thresholds = [300, 1000, 30000]\n","\n","sampling_params = vllm.SamplingParams(temperature=0.8, top_p=0.95, max_tokens=4096, seed=598)\n","\n","configs = [(preemption_threshold, preemption_upper_threshold) for preemption_threshold in preemption_thresholds for preemption_upper_threshold in preemption_upper_thresholds]\n","configs = configs[::-1]\n","\n","# SLICE CONFIGS TO TELL IT WHERE TO START\n","#configs = configs[1:]\n","\n","for index, (preemption_threshold, preemption_upper_threshold) in enumerate(configs):\n","  print(f\"Beginning experiment {index + 1}: {preemption_threshold}-{preemption_upper_threshold}\")\n","  # Check if outputpath/\n","  file_name = f\"preemption-threshold{preemption_threshold}_preemption-mode-upper-threshold{preemption_upper_threshold}.txt\"\n","  file_path = os.path.join(output_path, file_name)\n","\n","  if os.path.exists(file_path):\n","    print(f\"Experiment {index + 1} already exists. Skipping.\")\n","    continue\n","\n","  while True:\n","    try:\n","      llm = vllm.LLM(model=model, preemption_threshold=preemption_threshold, preemption_mode_upper_threshold=preemption_upper_threshold)\n","      break\n","    except:\n","      # LLM Creation failed. Trying to free memory.\n","      destroy_model_parallel()\n","      gc.collect()\n","      torch.distributed.destroy_process_group()\n","      torch.cuda.empty_cache()\n","      print(\"Successfully delete the llm pipeline and free the GPU memory!\")\n","\n","  # RUN EXPERIMENT HERE\n","  # Note: our vLLM implementation does our timing/saving for us\n","  try:\n","    outputs = llm.generate(prompts, sampling_params, filedir=output_path)\n","  except Exception as e:\n","    print(f\"FAILURE FOR EXPERIMENT {index+1}: {preemption_threshold}-{preemption_upper_threshold} ({e})\")\n","\n","  # Cleanup\n","  destroy_model_parallel()\n","  del llm\n","  gc.collect()\n","  torch.distributed.destroy_process_group()\n","  torch.cuda.empty_cache()\n","  print(\"Successfully delete the llm pipeline and free the GPU memory!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1100,"status":"ok","timestamp":1713215146867,"user":{"displayName":"Vishwa R","userId":"15875909042964774326"},"user_tz":240},"id":"lWboxobl82M4","outputId":"31648150-80c0-4b46-a0dd-e6d9ced3b8d0"},"outputs":[],"source":["destroy_model_parallel()\n","del llm\n","gc.collect()\n","torch.distributed.destroy_process_group()\n","torch.cuda.empty_cache()\n","print(\"Successfully delete the llm pipeline and free the GPU memory!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
